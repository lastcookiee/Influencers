{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 20      # Number of features in the state representation\n",
    "action_dim = 1000   # Number of possible influencer actions (recommendations)\n",
    "hidden_dim = 128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model (for Q-value prediction)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained weights loaded for fc1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\AppData\\Local\\Temp\\ipykernel_8860\\167075464.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_dict = torch.load(\"pretrained_recommendation_model.pth\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RL model\n",
    "rl_model = DQN(input_dim, action_dim, hidden_dim)\n",
    "\n",
    "# Load pre-trained weights (if available) for the shared layers (e.g., fc1)\n",
    "try:\n",
    "    pretrained_dict = torch.load(\"pretrained_recommendation_model.pth\")\n",
    "    model_dict = rl_model.state_dict()\n",
    "    # Update only keys related to the first layer (if the architectures match)\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and \"fc1\" in k}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    rl_model.load_state_dict(model_dict)\n",
    "    print(\"Pre-trained weights loaded for fc1.\")\n",
    "except Exception as e:\n",
    "    print(\"Pre-trained weights not loaded, using random initialization:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the target network for stability\n",
    "target_net = DQN(input_dim, action_dim, hidden_dim)\n",
    "target_net.load_state_dict(rl_model.state_dict())\n",
    "\n",
    "# Define optimizer and hyperparameters for RL training\n",
    "optimizer_rl = optim.Adam(rl_model.parameters(), lr=1e-4)\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "# We'll use a simple list as our replay buffer for this example\n",
    "replay_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple dummy environment (replace this with your actual environment)\n",
    "class RLDummyEnv:\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "    \n",
    "    def reset(self):\n",
    "        # Return an initial random state vector\n",
    "        return np.random.rand(self.state_dim)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Simulate an environment step:\n",
    "        # - next_state: random state (in practice, depends on action and current context)\n",
    "        # - reward: random reward (replace with your reward logic)\n",
    "        # - done: small probability to end the episode\n",
    "        next_state = np.random.rand(self.state_dim)\n",
    "        reward = np.random.rand()  # Replace with a meaningful reward function\n",
    "        done = random.random() < 0.05\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLDummyEnv(input_dim, action_dim)\n",
    "\n",
    "# Simple function to sample a minibatch from the replay buffer\n",
    "def sample_replay(buffer, batch_size):\n",
    "    return random.sample(buffer, batch_size)\n",
    "\n",
    "# RL training loop parameters\n",
    "num_episodes = 500\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Epsilon: 0.995\n",
      "Episode 10, Epsilon: 0.946\n",
      "Episode 20, Epsilon: 0.900\n",
      "Episode 30, Epsilon: 0.856\n",
      "Episode 40, Epsilon: 0.814\n",
      "Episode 50, Epsilon: 0.774\n",
      "Episode 60, Epsilon: 0.737\n",
      "Episode 70, Epsilon: 0.701\n",
      "Episode 80, Epsilon: 0.666\n",
      "Episode 90, Epsilon: 0.634\n",
      "Episode 100, Epsilon: 0.603\n",
      "Episode 110, Epsilon: 0.573\n",
      "Episode 120, Epsilon: 0.545\n",
      "Episode 130, Epsilon: 0.519\n",
      "Episode 140, Epsilon: 0.493\n",
      "Episode 150, Epsilon: 0.469\n",
      "Episode 160, Epsilon: 0.446\n",
      "Episode 170, Epsilon: 0.424\n",
      "Episode 180, Epsilon: 0.404\n",
      "Episode 190, Epsilon: 0.384\n",
      "Episode 200, Epsilon: 0.365\n",
      "Episode 210, Epsilon: 0.347\n",
      "Episode 220, Epsilon: 0.330\n",
      "Episode 230, Epsilon: 0.314\n",
      "Episode 240, Epsilon: 0.299\n",
      "Episode 250, Epsilon: 0.284\n",
      "Episode 260, Epsilon: 0.270\n",
      "Episode 270, Epsilon: 0.257\n",
      "Episode 280, Epsilon: 0.245\n",
      "Episode 290, Epsilon: 0.233\n",
      "Episode 300, Epsilon: 0.221\n",
      "Episode 310, Epsilon: 0.210\n",
      "Episode 320, Epsilon: 0.200\n",
      "Episode 330, Epsilon: 0.190\n",
      "Episode 340, Epsilon: 0.181\n",
      "Episode 350, Epsilon: 0.172\n",
      "Episode 360, Epsilon: 0.164\n",
      "Episode 370, Epsilon: 0.156\n",
      "Episode 380, Epsilon: 0.148\n",
      "Episode 390, Epsilon: 0.141\n",
      "Episode 400, Epsilon: 0.134\n",
      "Episode 410, Epsilon: 0.127\n",
      "Episode 420, Epsilon: 0.121\n",
      "Episode 430, Epsilon: 0.115\n",
      "Episode 440, Epsilon: 0.110\n",
      "Episode 450, Epsilon: 0.104\n",
      "Episode 460, Epsilon: 0.099\n",
      "Episode 470, Epsilon: 0.094\n",
      "Episode 480, Epsilon: 0.090\n",
      "Episode 490, Epsilon: 0.085\n",
      "Reinforcement Learning fine-tuning completed.\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = rl_model(state_tensor).argmax().item()\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Train the network if enough samples are available\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = sample_replay(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "            rewards = torch.FloatTensor(rewards).unsqueeze(1)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            # Convert boolean done flags to float (1.0 if done, 0.0 if not)\n",
    "            dones = torch.FloatTensor([1.0 if d else 0.0 for d in dones]).unsqueeze(1)\n",
    "            \n",
    "            # Compute current Q-values\n",
    "            q_values = rl_model(states).gather(1, actions)\n",
    "            # Compute next Q-values from the target network\n",
    "            next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "            # Compute the target Q-values using the Bellman equation\n",
    "            targets = rewards + gamma * next_q_values * (1 - dones)\n",
    "            \n",
    "            loss_rl = nn.MSELoss()(q_values, targets)\n",
    "            optimizer_rl.zero_grad()\n",
    "            loss_rl.backward()\n",
    "            optimizer_rl.step()\n",
    "    \n",
    "    # Decay epsilon after each episode\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Update target network periodically\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(rl_model.state_dict())\n",
    "        print(f\"Episode {episode}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "print(\"Reinforcement Learning fine-tuning completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
